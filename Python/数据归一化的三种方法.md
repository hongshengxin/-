#数据标准化（归一化）

&ensp;&ensp;**数据标准化（归一化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，
这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。
原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。以下是三种常用的归一化方法：**

##1. min-max标准化（Min-Max Normalization）

**也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 , 1]之间。转换函数如下：**

`x*=(x-min)/max-min   代码如下：`

``` python 
import numpy as np
arr = np.asarray([0, 10, 50, 80, 100])
for x in arr:
x = float(x - np.min(arr))/(np.max(arr)- np.min(arr))
print(x)
下面将数据缩至0-1之间，采用MinMaxScaler函数
from sklearn import preprocessing   
 
import numpy as np  
 
X = np.array([[ 1., -1.,  2.],  
 
              [ 2.,  0.,  0.],  
 
              [ 0.,  1., -1.]])  
 
min_max_scaler = preprocessing.MinMaxScaler()  
 
X_minMax = min_max_scaler.fit_transform(X) 
```

`目的为：	• 1、对于方差非常小的属性可以增强其稳定性；• 2、维持稀疏矩阵中为0的条目。`

## 2. Z-score标准化方法又称标准化

**也称为均值归一化(mean normaliztion)， 给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。
经过处理的数据符合标准正态分布，即均值为0，标准差为1。转化函数为**

![](https://img-blog.csdn.net/20160704143139785)


```
import numpy as np

arr = np.asarray([0, 10, 50, 80, 100])
for x in arr:
    x = float(x - arr.mean())/arr.std()
    print x
```

## 3. 正则化
正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。
 
Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数（l1-norm,l2-norm）等于1。
 
             p-范数的计算公式：||X||p=(|x1|^p+|x2|^p+...+|xn|^p)^1/p
该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。
```
X = [[ 1., -1., 2.],
[ 2., 0., 0.],
[ 0., 1., -1.]]
X_normalized = preprocessing.normalize(X, norm='l2')
X_normalized
array([[ 0.40..., -0.40..., 0.81...],
[ 1. ..., 0. ..., 0. ...],
[ 0. ..., 0.70..., -0.70...]])
```

















