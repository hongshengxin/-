#[信息熵、联合熵、条件熵、互信息](https://blog.csdn.net/bymaymay/article/details/85059136)

1. 自信息量
一个随机事件x xx的自信息量1定义为：
I(x)=log1p(x) I(x)=\log\frac{1}{p(x)}
I(x)=log 
p(x)
1
​	
 

注意，在信息论中，log \loglog函数的底通常设置为2，此时，自信息量的单位为比特(bit)；在机器学习中，log \loglog函数的底通常设置为自然常数e，此时，自信息量的单位为奈特(nat)。

需要从以下两方面来理解自信息量：

自信息量表示，如果随机事件x xx发生的概率p(x) p(x)p(x)越小，一旦其发生，所获得的信息量就越大
自信息量反映了事件发生的不确定性
举例说明，“中彩票”事件的概率极小，但是一旦中了彩票，“中彩票”事件的自信息量很大，也就是说，“中彩票”会获得极大的信息量（即收益）。另一方面，“中彩票”事件的概率很低，自信息量很大，意味着“中彩票”事件发生的不确定性也很大。

发生概率越高的事情，具有的自信息量越少
发生概率越低的事情，具有的自信息量越多
2. 信息熵
一个随机变量X XX的信息熵2定义为：
H(X)=∑xi∈Xp(xi)I(xi) =∑xi∈Xp(xi)log1p(xi).
H(X)=∑xi∈Xp(xi)I(xi) =∑xi∈Xp(xi)log⁡1p(xi).
简记为：
H(X)=−∑xp(x)logp(x). H(X)=-\sum_{x}p(x)\log p(x).
H(X)=− 
x
∑
​	
 p(x)logp(x).

信息熵的单位与自信息量一样。一个随机变量X XX可以有多种取值可能，信息熵是随机变量X XX所有可能情况的自信息量的期望。信息熵H(X) H(X)H(X)表征了随机变量X XX所有情况下的平均不确定度。

不确定度越大，信息量越大
不确定度越小，信息量越小
3. 最大熵定理
当随机变量X XX所有取值的概率相等时，即p(xi) p(x_i)p(x 
i
​	
 )的概率都相等时，信息熵取最大值，随机变量具有最大的不确定性。例如，情景一：买彩票中奖和不中奖的概率都是0.5 0.50.5时，此时买彩票是否中奖的不确定性最大。情景二：真实情况中，不中奖的概率远远大于中奖的概率，此时的不确定性要小于情景一，因为几乎能确定为不中奖。

最大熵定理
当随机变量X XX，在离散情况下所有取值概率相等（或在连续情况下服从均匀分布），此时熵最大。即0≤H(X)≤log∣X∣ 0\leq H(X)\leq \log |X|0≤H(X)≤log∣X∣，其中∣X∣ |X|∣X∣表示X XX的取值个数。
例1. 根据经验判断，买彩票中奖的概率是80% 80\%80%，不中奖的概率是20% 20\%20%，求买彩票的信息熵。

解： 买彩票的概率空间为：
(XP)=(x10.8x20.2) \binom{X}{P}=\begin{pmatrix}x_{1} &amp;x_{2} \\ 0.8 &amp; 0.2\end{pmatrix}
( 
P
X
​	
 )=( 
x 
1
​	
 
0.8
​	
  
x 
2
​	
 
0.2
​	
 )

其中，x1 x_{1}x 
1
​	
 表示买的彩票没奖，x2 x_{2}x 
2
​	
 表示买的彩票有奖。

买彩票后，“没中奖”事件获得的自信息量为：
I(x1)=log210.8=log21.25=log101.25log102=0.322 bit I(x_1)=\log_2\frac{1}{0.8}=\log_21.25=\frac{\log_{10}1.25}{\log_{10}2}=0.322~\text{bit}
I(x 
1
​	
 )=log 
2
​	
  
0.8
1
​	
 =log 
2
​	
 1.25= 
log 
10
​	
 2
log 
10
​	
 1.25
​	
 =0.322 bit
买彩票后，“中奖”事件获得的自信息量为：
I(x2)=log210.2=log25=log105log102=2.322 bit I(x_2)=\log_2\frac{1}{0.2}=\log_25=\frac{\log_{10}5}{\log_{10}2}=2.322~\text{bit}
I(x 
2
​	
 )=log 
2
​	
  
0.2
1
​	
 =log 
2
​	
 5= 
log 
10
​	
 2
log 
10
​	
 5
​	
 =2.322 bit
由I(x1)&lt;I(x2) I(x_1)&lt;I(x_2)I(x 
1
​	
 )<I(x 
2
​	
 )可知，彩票有奖的不确定性要大于彩票没奖。

买彩票的信息熵为：
H(X)=p(x1)I(x1)+p(x2)I(x2)=0.8∗0.322+0.2∗2.322=0.722 bit H(X)=p(x_1)I(x_1)+p(x_2)I(x_2)=0.8*0.322+0.2*2.322=0.722~\text{bit}
H(X)=p(x 
1
​	
 )I(x 
1
​	
 )+p(x 
2
​	
 )I(x 
2
​	
 )=0.8∗0.322+0.2∗2.322=0.722 bit

**结果分析：**由最大熵定理可知，信息熵H(X) H(X)H(X)的最大值为H(X)max=−log1/2=1 H(X)_{\max}=-\log 1/2=1H(X) 
max
​	
 =−log1/2=1。例1 11中H(X) H(X)H(X)小于1比特，意味着不确定性减少，带来的信息量也减少。也就是说，先验经验（买彩票大概率不中奖）减少了不确定性。

4. 联合熵
随机变量X XX和Y YY的联合熵定义为：
H(X,Y)=∑xi∈X∑yi∈Yp(xi,yi)I(xi,yi) =∑xi∈X∑yi∈Yp(xi,yi)log1p(xi,yi)
H(X,Y)=∑xi∈X∑yi∈Yp(xi,yi)I(xi,yi) =∑xi∈X∑yi∈Yp(xi,yi)log1p(xi,yi)
简记为：
H(X,Y)=−∑x,yp(x,y)logp(x,y) H(X, Y)=-\sum_{x,y}p(x,y)\log p(x,y)
H(X,Y)=− 
x,y
∑
​	
 p(x,y)logp(x,y)

**联合熵H(X,Y) H(X, Y)H(X,Y)表示随机变量X XX和Y YY一起发生时的信息熵，即X XX和Y YY一起发生时的确定度。**通俗地讲，联合熵H(X,Y) H(X, Y)H(X,Y)表示X XX和Y YY一起发生时，产生的信息量。

5. 条件熵H(X∣Y) H(X|Y)H(X∣Y)
随机变量X XX和Y YY的**条件熵H(Y∣X) H(Y|X)H(Y∣X)**定义为：
H(X∣Y)=∑yj∈Yp(yj)H(X∣Y=yj) H(X|Y)=\sum_{y_j\in Y}p(y_j)H(X|Y=y_j)
H(X∣Y)= 
y 
j
​	
 ∈Y
∑
​	
 p(y 
j
​	
 )H(X∣Y=y 
j
​	
 )

**条件熵H(X∣Y) H(X|Y)H(X∣Y)表示已知随机变量Y YY的情况下，随机变量X XX的信息熵，即在Y YY发生的前提下，X XX发生后新带来的不确定度。**通俗地讲，条件熵H(X∣Y) H(X|Y)H(X∣Y)表示在Y YY发生的前提下，X XX发生新带来的信息量。

具体使用形式为：
H(X|Y)=∑yj∈Yp(yj)H(X|Y=yj) =−∑yj∈Yp(yj)∑xi∈Xp(xi|yj)logp(xi|yj) =−∑yj∈Y∑xi∈Xp(yj)p(xi|yj)logp(xi|yj) =−∑xi,yjp(xi,yj)logp(xi|yj)
H(X|Y)=∑yj∈Yp(yj)H(X|Y=yj) =−∑yj∈Yp(yj)∑xi∈Xp(xi|yj)log⁡p(xi|yj) =−∑yj∈Y∑xi∈Xp(yj)p(xi|yj)log⁡p(xi|yj) =−∑xi,yjp(xi,yj)log⁡p(xi|yj)
简记为：
H(X∣Y)=−∑x,yp(x,y)logp(x∣y) H(X|Y)=-\sum_{x,y}p(x,y)\log p(x|y)
H(X∣Y)=− 
x,y
∑
​	
 p(x,y)logp(x∣y)

条件熵H(X∣Y) H(X|Y)H(X∣Y)与联合熵H(X,Y) H(X,Y)H(X,Y)的关系为：
H(X∣Y)=H(X,Y)−H(Y) H(X|Y)=H(X,Y)-H(Y)
H(X∣Y)=H(X,Y)−H(Y)

推导过程如下：
H(X|Y)=−∑x,yp(x,y)logp(x|y) =−∑x,yp(x,y)logp(x,y)p(y) =−∑x,yp(x,y)logp(x,y)+∑x,yp(x,y)logp(y) =−∑x,yp(x,y)logp(x,y)+∑y（∑xp(x,y)）logp(y) =−∑x,yp(x,y)logp(x,y)+∑yp(y)logp(y) =H(X,Y)−H(Y)
H(X|Y)=−∑x,yp(x,y)log⁡p(x|y) =−∑x,yp(x,y)log⁡p(x,y)p(y) =−∑x,yp(x,y)log⁡p(x,y)+∑x,yp(x,y)log⁡p(y) =−∑x,yp(x,y)log⁡p(x,y)+∑y（∑xp(x,y)）log⁡p(y) =−∑x,yp(x,y)log⁡p(x,y)+∑yp(y)log⁡p(y) =H(X,Y)−H(Y)
5. 条件熵H(Y∣X) H(Y|X)H(Y∣X)
随机变量X XX和Y YY的**条件熵H(Y∣X) H(Y|X)H(Y∣X)**定义为：
H(Y∣X)=∑xi∈Xp(xi)H(Y∣X=xi) H(Y|X)=\sum_{x_i\in X}p(x_i)H(Y|X=x_i)
H(Y∣X)= 
x 
i
​	
 ∈X
∑
​	
 p(x 
i
​	
 )H(Y∣X=x 
i
​	
 )

**条件熵H(Y∣X) H(Y|X)H(Y∣X)表示已知随机变量X XX的情况下，随机变量Y YY的信息熵，即在X XX发生的前提下，Y YY发生后新带来的不确定度。**通俗地讲，条件熵H(Y∣X) H(Y|X)H(Y∣X)表示在X XX发生的前提下，Y YY发生新带来的信息量。

具体使用形式为：
H(Y|X)=∑xi∈Xp(xi)H(Y|X=xi) =−∑xi∈Xp(xi)∑yj∈Yp(yj|xi)logp(yj|xi) =−∑xi∈X∑yj∈Yp(xi)p(yj|xi)logp(yj|xi) =−∑xi,yjp(xi,yj)logp(yj|xi)
H(Y|X)=∑xi∈Xp(xi)H(Y|X=xi) =−∑xi∈Xp(xi)∑yj∈Yp(yj|xi)log⁡p(yj|xi) =−∑xi∈X∑yj∈Yp(xi)p(yj|xi)log⁡p(yj|xi) =−∑xi,yjp(xi,yj)log⁡p(yj|xi)
简记为：
H(Y∣X)=−∑x,yp(x,y)logp(y∣x) H(Y|X)=-\sum_{x,y}p(x,y)\log p(y|x)
H(Y∣X)=− 
x,y
∑
​	
 p(x,y)logp(y∣x)

条件熵H(Y∣X) H(Y|X)H(Y∣X)与联合熵H(X,Y) H(X,Y)H(X,Y)的关系为：
H(Y∣X)=H(X,Y)−H(X) H(Y|X)=H(X,Y)-H(X)
H(Y∣X)=H(X,Y)−H(X)

推导过程见H(X∣Y) H(X|Y)H(X∣Y)。

7. 互信息
互信息量定义为后验概率与先验概率比值的对数：
I(xi;yj)=logp(xi∣yj)p(xi) I(x_i;y_j)=\log \frac{p(x_i|y_j)}{p(x_i)}
I(x 
i
​	
 ;y 
j
​	
 )=log 
p(x 
i
​	
 )
p(x 
i
​	
 ∣y 
j
​	
 )
​	
 

互信息（平均互信息量）：
I(X;Y)=∑xi∈X∑yj∈Yp(xi,yj)logp(xi∣yj)p(xi) I(X;Y)=\sum_{x_i\in X}\sum_{y_j \in Y}p(x_i,y_j)\log \frac{p(x_i|y_j)}{p(x_i)}
I(X;Y)= 
x 
i
​	
 ∈X
∑
​	
  
y 
j
​	
 ∈Y
∑
​	
 p(x 
i
​	
 ,y 
j
​	
 )log 
p(x 
i
​	
 )
p(x 
i
​	
 ∣y 
j
​	
 )
​	
 

简记为：
I(X;Y)=∑x,yp(x,y)logp(x∣y)p(x) I(X;Y)=\sum_{x,y}p(x,y)\log \frac{p(x|y)}{p(x)}
I(X;Y)= 
x,y
∑
​	
 p(x,y)log 
p(x)
p(x∣y)
​	
 

互信息具有以下性质：
I(X;Y)=H(X)−H(X|Y) =H(Y)−H(Y|X) =I(Y;X)
I(X;Y)=H(X)−H(X|Y) =H(Y)−H(Y|X) =I(Y;X)
互信息的理解：
H(X) H(X)H(X)是X XX的不确定度，H(X∣Y) H(X|Y)H(X∣Y)是Y YY已知时是X XX的不确定度，则I(X;Y)=H(X)−H(X∣Y) I(X;Y)=H(X)-H(X|Y)I(X;Y)=H(X)−H(X∣Y)表示Y YY已知使得X XX的不确定度减少了I(X;Y) I(X;Y)I(X;Y)。Y YY已知时X XX的不确定度为H(X∣Y)=H(X)−I(X;Y) H(X|Y)=H(X)-I(X;Y)H(X∣Y)=H(X)−I(X;Y)。

8. 小结
名称	公式	含义
熵H(X) H(X)H(X)	H(X)=−∑x∈Xp(x)logp(x) H(X)=-\sum_{x\in X}p(x)\log p(x)H(X)=−∑ 
x∈X
​	
 p(x)logp(x)	熵H(X) H(X)H(X)表示X XX的不确定度
联合熵H(X,Y) H(X, Y)H(X,Y)	H(X,Y)=−∑x,yp(x,y)logp(x,y) H(X, Y)=-\sum_{x,y}p(x,y)\log p(x,y)H(X,Y)=−∑ 
x,y
​	
 p(x,y)logp(x,y)	联合熵H(X,Y) H(X, Y)H(X,Y)表示X XX和Y YY一起发生的不确定度
条件熵$H(Y	X)$	$H(Y
条件熵$H(X	Y)$	$H(X
互信息I(X;Y) I(X;Y)I(X;Y)	\begin{align*}	
I(X;Y) &= H(X)-H(X	Y) \	
I(Y;X) &= H(Y)-H(Y	X)\	
I(X;Y) &= I(Y;X)
\end{align*}| 互信息I(X;Y) I(X;Y)I(X;Y)表示Y YY发生后，X XX的不确定度减少了I(X;Y) I(X;Y)I(X;Y)|

关系图：

--------------------- 
作者：bymaymay 
来源：CSDN 
原文：https://blog.csdn.net/bymaymay/article/details/85059136 
版权声明：本文为博主原创文章，转载请附上博文链接！