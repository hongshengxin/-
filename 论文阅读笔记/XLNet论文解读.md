


# 目的

###技术路线

* 自回归

```
AR LM，即自回归语言模型。具体而言，给定一个序列，当前token/时刻只知道前面的信息，
而不知道后面的信息，即使分成正向和反向计算当前token时刻的概率分布，
也是同样的原则，ELMo、GPT是属于这个范畴。对于一些自然语言理解任务而言，是给定上下文的，
即使ELMo把两个的方向计算的信息concat，但也是独立计算，对上下文的编码是有缺陷的。
```

* 自编码

```
AE LM，即自编码语言模型。BERT通过预测原始数据里MASK掉的token来预训练语言模型，
预测[MASK]使用了上下文信息，弥补了AR LM的缺陷。但是[MASK]只在预训练的时候用到，
finetune的时候是不用的，这使得pretrain/train不一致【这点顶一下BERT，我觉得这样更能体现泛化能力】。并且，BERT假定每个[MASK]与其他[MASK]是相互独立的，不能计算序列、长期依赖的联合概率
。即使BERT的NSP预训练任务一定程度上给了模型建模句间关系的能力，但是还是对长文本不敏感。
```

