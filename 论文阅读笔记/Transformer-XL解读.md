#Transformer-XL解读

这篇划时代的文章是由CMU和谷歌联手发布一篇论文，介绍了一种新的语言建模方法Transformer-XL。

这里的XL，指的是extra long，意思是超长，表示Transformer-XL在语言建模中长距离依赖问题上
有非常好的表现。同时，也暗示着它就是为长距离依赖问题而生。

* 长距离依赖问题，是当前文本处理模型面临的难题，也是RNN失败的地方。
* 打破char级语言建模1.0障碍的工具

相比之下，Transformer-XL学习的依赖要比RNN长80%。比Vanilla Transformers快450%。

在短序列和长序列上，都有很好的性能表现。更可怕的还在速度上，在评估过程中，比Vanilla Transformers快了1800倍以上。


###一. 回顾Transformer

Transformer 网络具有学习更长期依赖性的潜力，但这种潜力往往会受到语言建模中上下文长度固定的限制。
用 Transformer 或自注意力机制进行语言建模的核心问题在于，如何将 Transformer 训练为可以把任意长度的上下文有效编码为长度固定的表征。在给定无限内存和计算资源的情况下，一种简单的方法是用无条件的 Transformer 解码器处理整个上下文序列，这和前馈神经网络相似。
但在实践中资源都是有限的，因此这种方法不可行。

###二. xl的思路

####Transformer的局限

在将Transformer或者是自注意力机制（self-attention）应用到语言建模中，需要解决的核心问题是如何训练Transformer，将任意长度的语境有效地编码为固定大小的表征。

如果有无限的存储和计算资源，一个无条件的Transformer就能解决这个问题。但在实际的运用中，资源是有限的，这个思路就行不通了。

另一个思路，就是将序列分成可以管理的较短片段，在每个片段内训练模型，忽略来自先前片段的所有语境信息，如下图的中a部分所示。
![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCvQa0fj7FXq2Is7fC7eXCiccyx3iaw45A7l8aibYWQy0A3p7jNsanyxiaUGobDOQJfjmqztpv0C1wmqw/640?wx_fmt=png)

这就是之前Rami Al-Rfou（谷歌AI高级软件工程师）等人采用的方法。

在这种训练范式下，信息不可能向前或向后穿过各个部分进行传递，这也就带来了新的麻烦。

首先，Transformer最大可能的依赖长度，将受到每个片段的长度限制。字符级的语言建模上通常是几百。所以，尽管与RNN相比，Transformer受梯度消失问题的影响较小，但在Vanilla 模型中，并不能充分利用这个优势。

而且，简单地将语料库分割成固定长度的片段，也会导致语境碎片的问题。

如上图中b部分所示，在评估期间的每个步骤，Vanilla模型也将会消耗与训练中相同长度的片段，但仅仅在最后位置进行一次预测。在下一步，的这个片段仅仅只是向右移动了一个位置，然后又从头处理新的片段。

虽然这样做有助于确保进行每个预测的时候，利用训练期间暴露的最长可能语境，还能减轻训练中遇到的语境碎片问题。但真的是太贵了。

####引入重用机制

为了突破使用固定长度片段带来的语境限制，这篇论文在Transformer体系结构中引入了重复机制。

在训练期间，为模型处理下一个新的片段时，会缓存前一个片段计算的隐藏状态序列，并作为扩展语境重用，如下图中所示。

![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCvQa0fj7FXq2Is7fC7eXCicz2GAgeZvcq4icyz2Mr6YY5ApF2liaP4kKX5cPHr6I99USoHqD7HQYOicQ/640?wx_fmt=png)

虽然梯度仍然保留在一个片段内，但这个额外的输入，允许网络利用历史信息，从而能够对长期依赖建模，并避免场景碎片。

除了这些好处之外，重复机制还能够加快评估速度。在评估期间，可以重复使用来自先前片段的表征，而不是像Vanilla模型从头开始。

####相对位置编码

但是，想要重用隐藏状态，还需要解决一个关键的技术挑战：重用状态时，如何保持位置信息的一致性？

在标准的Transformer中，序列顺序的信息，都是由一组位置编码提供，每一个位置都有绝对的位置信息。但将这个逻辑应用到重用机制中时，会导致性能损失。

这个问题的解决思路是，对隐藏状态中的相对位置信息进行编码。从概念上讲，位置编码为模型提供了关于应如何收集信息的时间线索，即应该在哪里介入处理。

以相对的方式定义时间线索，将相同的信息注入每层的注意分数，更加直观，也更通用。

基于这个思路，可以创建一组相对位置编码，使得重用机制变得可行，也不会丢失任何的时间信息。

将相对位置嵌入Transformer之中，并配合重用机制，就得到了Transformer-XL的架构。

基于这些改进，Transformer-XL在相关的数据集上都取得了很好的成绩。论文中表示，这是第一个在字符级和单词级建模方面比RNN结果更好的自注意力模型。

## 三.源码解读



